{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5716e010-4e4e-4bf8-9635-029b12fba110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded shape: (5000, 15)\n",
      "Using target column: Risk_Level\n",
      "\n",
      "Training LogisticRegression...\n",
      "CV macro-F1 scores: [0.95350411 0.96599333 0.96445071 0.95823217 0.95355663], mean=0.9591\n",
      "Test macro F1: 0.9631, Balanced accuracy: 0.9591\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    High Risk       1.00      0.93      0.96       241\n",
      "Moderate Risk       0.99      0.95      0.97       384\n",
      "         Safe       0.92      1.00      0.96       375\n",
      "\n",
      "     accuracy                           0.96      1000\n",
      "    macro avg       0.97      0.96      0.96      1000\n",
      " weighted avg       0.97      0.96      0.96      1000\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "               Pred_Safe  Pred_Moderate  Pred_High\n",
      "True_Safe            375              0          0\n",
      "True_Moderate         20            364          0\n",
      "True_High             12              5        224\n",
      "\n",
      "Training RandomForest...\n",
      "CV macro-F1 scores: [0.95710432 0.96458566 0.96693557 0.95820067 0.9557069 ], mean=0.9605\n",
      "Test macro F1: 0.9631, Balanced accuracy: 0.9591\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    High Risk       1.00      0.93      0.96       241\n",
      "Moderate Risk       0.99      0.95      0.97       384\n",
      "         Safe       0.92      1.00      0.96       375\n",
      "\n",
      "     accuracy                           0.96      1000\n",
      "    macro avg       0.97      0.96      0.96      1000\n",
      " weighted avg       0.97      0.96      0.96      1000\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "               Pred_Safe  Pred_Moderate  Pred_High\n",
      "True_Safe            375              0          0\n",
      "True_Moderate         20            364          0\n",
      "True_High             12              5        224\n",
      "\n",
      "Training GradientBoosting...\n",
      "CV macro-F1 scores: [0.96904026 0.9754281  0.97592546 0.9726868  0.96444324], mean=0.9715\n",
      "Test macro F1: 0.9724, Balanced accuracy: 0.9700\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    High Risk       1.00      0.95      0.97       241\n",
      "Moderate Risk       0.99      0.96      0.98       384\n",
      "         Safe       0.94      1.00      0.97       375\n",
      "\n",
      "     accuracy                           0.97      1000\n",
      "    macro avg       0.98      0.97      0.97      1000\n",
      " weighted avg       0.97      0.97      0.97      1000\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "               Pred_Safe  Pred_Moderate  Pred_High\n",
      "True_Safe            374              0          1\n",
      "True_Moderate         16            368          0\n",
      "True_High              9              2        230\n",
      "\n",
      "Top 10 RandomForest features:\n",
      "                  feature  importance\n",
      "18          Stress_Medium    0.288048\n",
      "17             Stress_Low    0.276015\n",
      "16            Stress_High    0.272474\n",
      "3             Sleep_Hours    0.027622\n",
      "2              Test_Score    0.024579\n",
      "0              Attendance    0.021057\n",
      "4             Screen_Time    0.016356\n",
      "1   Assignment_Timeliness    0.016000\n",
      "5               LMS_Hours    0.015118\n",
      "7              Peer_Score    0.009515\n",
      "\n",
      "Computing permutation importance on test set for best model...\n",
      "Permutation importance failed: All arrays must be of the same length\n",
      "\n",
      "Saved predictions to: data\\predictions_with_labels.csv\n",
      "Saved best model (GradientBoosting) to: data\\best_model.pkl\n",
      "Saved class distribution plot to: data\\class_distribution.png\n",
      "\n",
      "Intervention recommendations:\n",
      "- Safe: General well-being resources; periodic check-ins; encourage healthy study-life balance.\n",
      "- Moderate Risk: Early counseling, stress-management workshops, parental + teacher notification, monitor academic load.\n",
      "- High Risk: Immediate counseling referral, mental health evaluation, crisis intervention plan, involve guardians and healthcare professionals.\n",
      "Saved pipeline summary to: data\\pipeline_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import sklearn\n",
    "\n",
    "# CONFIG\n",
    "DATA_PATH = \"student_wellbeing_dataset.csv\"  # set to your file path if different\n",
    "OUTPUT_DIR = \"data\"                          # directory where outputs will be saved\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Data file not found at: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def detect_and_prepare_target(df: pd.DataFrame) -> Tuple[pd.DataFrame, str]:\n",
    "    possible_targets = [c for c in df.columns if c.lower() in\n",
    "                        [\"target\", \"label\", \"wellbeing\", \"well_being\", \"risk\", \"category\",\n",
    "                         \"risk_level\", \"wellbeing_category\", \"status\"]]\n",
    "    target_col = possible_targets[0] if possible_targets else None\n",
    "\n",
    "    if not target_col:\n",
    "        candidates = [c for c in df.columns if df[c].nunique() <= 5]\n",
    "        if not candidates:\n",
    "            raise ValueError(\"No obvious low-cardinality target column found. Please specify the target column.\")\n",
    "        target_col = candidates[0]\n",
    "\n",
    "    # Normalize and map common forms to exact labels\n",
    "    vals = df[target_col].dropna().astype(str).str.strip()\n",
    "    vals_normal = vals.str.lower().str.replace(\"_\", \" \").str.replace(\"-\", \" \")\n",
    "\n",
    "    mapping = {\n",
    "        'moderate': 'Moderate Risk',\n",
    "        'moderate risk': 'Moderate Risk',\n",
    "        'moderaterisk': 'Moderate Risk',\n",
    "        'med': 'Moderate Risk',\n",
    "        'high': 'High Risk',\n",
    "        'high risk': 'High Risk',\n",
    "        'highrisk': 'High Risk',\n",
    "        'low': 'Safe',\n",
    "        'safe': 'Safe'\n",
    "    }\n",
    "\n",
    "    mapped = vals_normal.map(mapping)\n",
    "    # where mapping failed, keep original trimmed value\n",
    "    df[target_col] = mapped.fillna(vals)\n",
    "\n",
    "    df = df[~df[target_col].isnull()].reset_index(drop=True)\n",
    "    return df, target_col\n",
    "\n",
    "\n",
    "def split_features_target(df: pd.DataFrame, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    id_like = [c for c in X.columns if c.lower().endswith('id') or c.lower() in ['id', 'student_id', 'sid']]\n",
    "    if id_like:\n",
    "        X = X.drop(columns=id_like)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def build_preprocessor(X: pd.DataFrame) -> Tuple[ColumnTransformer, list, list]:\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # compatibility with sklearn versions\n",
    "    if sklearn.__version__ >= \"1.2\":\n",
    "        onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    else:\n",
    "        onehot = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', onehot)\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ], remainder='drop')\n",
    "\n",
    "    return preprocessor, numeric_cols, categorical_cols\n",
    "\n",
    "\n",
    "def train_and_evaluate(X: pd.DataFrame, y: pd.Series, preprocessor: ColumnTransformer,\n",
    "                       numeric_cols: list, categorical_cols: list) -> Dict[str, Any]:\n",
    "    models = {\n",
    "        'LogisticRegression': Pipeline(steps=[('pre', preprocessor),\n",
    "                                              ('clf', LogisticRegression(max_iter=1000, class_weight='balanced',\n",
    "                                                                         random_state=RANDOM_STATE))]),\n",
    "        'RandomForest': Pipeline(steps=[('pre', preprocessor),\n",
    "                                        ('clf', RandomForestClassifier(n_estimators=200, class_weight='balanced',\n",
    "                                                                       random_state=RANDOM_STATE))]),\n",
    "        'GradientBoosting': Pipeline(steps=[('pre', preprocessor),\n",
    "                                            ('clf', GradientBoostingClassifier(n_estimators=200,\n",
    "                                                                               random_state=RANDOM_STATE))])\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "    for name, pipe in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='f1_macro')\n",
    "        print(f\"CV macro-F1 scores: {scores}, mean={scores.mean():.4f}\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        preds = pipe.predict(X_test)\n",
    "        report = classification_report(y_test, preds, zero_division=0)\n",
    "        cm = confusion_matrix(y_test, preds, labels=['Safe', 'Moderate Risk', 'High Risk'])\n",
    "        f1 = f1_score(y_test, preds, average='macro', zero_division=0)\n",
    "        bal_acc = balanced_accuracy_score(y_test, preds)\n",
    "\n",
    "        print(f\"Test macro F1: {f1:.4f}, Balanced accuracy: {bal_acc:.4f}\")\n",
    "        print(report)\n",
    "        print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "        print(pd.DataFrame(cm, index=['True_Safe', 'True_Moderate', 'True_High'],\n",
    "                           columns=['Pred_Safe', 'Pred_Moderate', 'Pred_High']))\n",
    "\n",
    "        results[name] = {\n",
    "            'pipeline': pipe,\n",
    "            'cv_scores': scores,\n",
    "            'test_f1_macro': f1,\n",
    "            'balanced_accuracy': bal_acc,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "    best_name = max(results.keys(), key=lambda k: results[k]['test_f1_macro'])\n",
    "    best_model = results[best_name]['pipeline']\n",
    "    results['best'] = {'name': best_name, 'model': best_model}\n",
    "\n",
    "    # RandomForest feature importances\n",
    "    if 'RandomForest' in results:\n",
    "        try:\n",
    "            rf_pipe = results['RandomForest']['pipeline']\n",
    "            pre = rf_pipe.named_steps['pre']\n",
    "            cat_features = pre.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols).tolist() if categorical_cols else []\n",
    "            feature_names = numeric_cols + cat_features\n",
    "            rf = rf_pipe.named_steps['clf']\n",
    "            importances = rf.feature_importances_\n",
    "            fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "            results['RandomForest']['feature_importances'] = fi_df\n",
    "            print(\"\\nTop 10 RandomForest features:\")\n",
    "            print(fi_df.head(10))\n",
    "        except Exception as e:\n",
    "            print(\"Could not extract RandomForest importances:\", e)\n",
    "\n",
    "    # Permutation importance on best model\n",
    "    try:\n",
    "        print(\"\\nComputing permutation importance on test set for best model...\")\n",
    "        r = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, scoring='f1_macro')\n",
    "        pre = best_model.named_steps['pre']\n",
    "        cat_features = pre.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols).tolist() if categorical_cols else []\n",
    "        feat_names = numeric_cols + cat_features\n",
    "        perm_df = pd.DataFrame({'feature': feat_names, 'importance_mean': r.importances_mean}).sort_values('importance_mean', ascending=False)\n",
    "        results['permutation_importance'] = perm_df\n",
    "        print(perm_df.head(10))\n",
    "    except Exception as e:\n",
    "        print(\"Permutation importance failed:\", e)\n",
    "\n",
    "    # Save predictions\n",
    "    preds_full = best_model.predict(X)\n",
    "    out_df = X.copy()\n",
    "    out_df['true_label'] = y.values\n",
    "    out_df['predicted_label'] = preds_full\n",
    "    preds_path = os.path.join(OUTPUT_DIR, 'predictions_with_labels.csv')\n",
    "    out_df.to_csv(preds_path, index=False)\n",
    "    print(f\"\\nSaved predictions to: {preds_path}\")\n",
    "\n",
    "    # Save model\n",
    "    model_path = os.path.join(OUTPUT_DIR, 'best_model.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    print(f\"Saved best model ({best_name}) to: {model_path}\")\n",
    "\n",
    "    # Class distribution plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    y.value_counts().plot(kind='bar')\n",
    "    plt.title('Class distribution in dataset')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt_path = os.path.join(OUTPUT_DIR, 'class_distribution.png')\n",
    "    plt.savefig(plt_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved class distribution plot to: {plt_path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    df = load_data(DATA_PATH)\n",
    "    print(f\"Loaded shape: {df.shape}\")\n",
    "\n",
    "    df, target_col = detect_and_prepare_target(df)\n",
    "    print(f\"Using target column: {target_col}\")\n",
    "\n",
    "    X, y = split_features_target(df, target_col)\n",
    "    preprocessor, numeric_cols, categorical_cols = build_preprocessor(X)\n",
    "\n",
    "    results = train_and_evaluate(X, y, preprocessor, numeric_cols, categorical_cols)\n",
    "\n",
    "    recommendations = {\n",
    "        'Safe': 'General well-being resources; periodic check-ins; encourage healthy study-life balance.',\n",
    "        'Moderate Risk': 'Early counseling, stress-management workshops, parental + teacher notification, monitor academic load.',\n",
    "        'High Risk': 'Immediate counseling referral, mental health evaluation, crisis intervention plan, involve guardians and healthcare professionals.'\n",
    "    }\n",
    "\n",
    "    print('\\nIntervention recommendations:')\n",
    "    for k, v in recommendations.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "    summary = {\n",
    "        'dataset_shape': [df.shape],\n",
    "        'target_column': [target_col],\n",
    "        'best_model': [results['best']['name']],\n",
    "        'best_model_test_macro_f1': [results[results['best']['name']]['test_f1_macro']]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_path = os.path.join(OUTPUT_DIR, 'pipeline_summary.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"Saved pipeline summary to: {summary_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f858f-f70b-4143-a406-7155102c7eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
